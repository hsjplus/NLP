{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "df_train = pd.read_csv('100 data/ratings_test.txt', delimiter='\\t', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  굳 ㅋ\n",
       "1                                 GDNTOPCLASSINTHECLUB\n",
       "2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
       "3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
       "4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
       "5                                   음악이 주가 된, 최고의 음악영화\n",
       "6                                              진정한 쓰레기\n",
       "7             마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
       "8    갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...\n",
       "9       이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['document'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "twitter_tag = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 형태소 분석 방법(조사, 어미 등 제외)\n",
    "def twitter_tokenizer_part(text):\n",
    "    lst = []\n",
    "    for tpl in twitter_tag.pos(text, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            lst.append(tpl[0])\n",
    "    if len(lst) != 0: \n",
    "        return lst\n",
    "    else:\n",
    "        return [\"뷁\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df_train['document'][:100]\n",
    "vect = CountVectorizer(tokenizer=twitter_tokenizer_part).fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['괜히', '굉장하다', '교훈', '구성', '군', '굳다', '굿굿', '궁금', '귀엽다', '그']\n",
      "622\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[80:90])\n",
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_words = []\n",
    "for line in text:\n",
    "    for tpl in twitter_tag.pos(line, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            tokenized_words.append(tpl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['점', '짜다', '리', '더', '더욱', '아니다', '지루하다', '않다', '완전', '막장']\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_words[10:20])\n",
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(len(tokenized_words)-3):\n",
    "    x_train.append(vect.transform([tokenized_words[i],tokenized_words[i+1],tokenized_words[i+2]]).toarray())\n",
    "    y_train.append(vect.transform([tokenized_words[i+3]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 622)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 단어 representation을 622차원에서 2차원으로 축소\n",
    "X = tf.placeholder(tf.float32, [3,622]) #x_train의 각 값을 넣을 것임 \n",
    "W = tf.Variable(tf.random_normal([622, 2], seed=0))\n",
    "b = tf.Variable(tf.random_normal([3,2], seed=0))\n",
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55631971  4.9211874   1.29128885  0.70431834 -3.6267972  -0.35812882]\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "for i in range(len(x_train)):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #print(\"=== W ===\")\n",
    "    #print(sess.run(W))\n",
    "    #print(\"=== b ===\")\n",
    "    #print(sess.run(b))\n",
    "    x_data.append(sess.run(expr, feed_dict={X: x_train[i]}).flatten())\n",
    "\n",
    "print(x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 622)               80238     \n",
      "=================================================================\n",
      "Total params: 81,134\n",
      "Trainable params: 81,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input([6,]) # input node는 2 * 3 = 6임\n",
    "h = Dense(128, activation='tanh')(x) # hidden node는 임의의 2의 승수 값으로 지정\n",
    "y = Dense(622, activation='softmax')(h) # output node는 one-hot encoding 차원인 640\n",
    "model = Model(x,y)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data를 array 형태로 변형\n",
    "import numpy as np\n",
    "x_data = np.array(x_data).reshape(len(x_data),6)\n",
    "y_train = np.array(y_train).reshape(len(y_train),622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 6)\n",
      "(978, 622)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "978/978 [==============================] - 1s 775us/step - loss: 6.3580 - acc: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "978/978 [==============================] - 0s 421us/step - loss: 6.3373 - acc: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "978/978 [==============================] - 0s 465us/step - loss: 6.3176 - acc: 0.0010\n",
      "Epoch 4/1000\n",
      "978/978 [==============================] - 0s 435us/step - loss: 6.2979 - acc: 0.0102\n",
      "Epoch 5/1000\n",
      "978/978 [==============================] - 0s 501us/step - loss: 6.2788 - acc: 0.0123\n",
      "Epoch 6/1000\n",
      "978/978 [==============================] - 0s 464us/step - loss: 6.2604 - acc: 0.0215\n",
      "Epoch 7/1000\n",
      "978/978 [==============================] - 0s 481us/step - loss: 6.2426 - acc: 0.0256\n",
      "Epoch 8/1000\n",
      "978/978 [==============================] - 0s 503us/step - loss: 6.2250 - acc: 0.0235\n",
      "Epoch 9/1000\n",
      "978/978 [==============================] - 0s 508us/step - loss: 6.2079 - acc: 0.0245\n",
      "Epoch 10/1000\n",
      "978/978 [==============================] - 0s 479us/step - loss: 6.1916 - acc: 0.0225\n",
      "Epoch 11/1000\n",
      "978/978 [==============================] - 0s 436us/step - loss: 6.1758 - acc: 0.0276\n",
      "Epoch 12/1000\n",
      "978/978 [==============================] - 0s 450us/step - loss: 6.1604 - acc: 0.0235\n",
      "Epoch 13/1000\n",
      "978/978 [==============================] - 0s 437us/step - loss: 6.1458 - acc: 0.0256\n",
      "Epoch 14/1000\n",
      "978/978 [==============================] - 0s 491us/step - loss: 6.1316 - acc: 0.0256\n",
      "Epoch 15/1000\n",
      "978/978 [==============================] - 0s 457us/step - loss: 6.1186 - acc: 0.0256\n",
      "Epoch 16/1000\n",
      "978/978 [==============================] - 0s 465us/step - loss: 6.1064 - acc: 0.0266\n",
      "Epoch 17/1000\n",
      "978/978 [==============================] - 0s 479us/step - loss: 6.0949 - acc: 0.0266\n",
      "Epoch 18/1000\n",
      "978/978 [==============================] - 0s 479us/step - loss: 6.0842 - acc: 0.0266\n",
      "Epoch 19/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 6.0744 - acc: 0.0266\n",
      "Epoch 20/1000\n",
      "978/978 [==============================] - 0s 505us/step - loss: 6.0654 - acc: 0.0245\n",
      "Epoch 21/1000\n",
      "978/978 [==============================] - 0s 490us/step - loss: 6.0569 - acc: 0.0256\n",
      "Epoch 22/1000\n",
      "978/978 [==============================] - 0s 480us/step - loss: 6.0492 - acc: 0.0256\n",
      "Epoch 23/1000\n",
      "978/978 [==============================] - 0s 492us/step - loss: 6.0418 - acc: 0.0245\n",
      "Epoch 24/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 6.0349 - acc: 0.0245\n",
      "Epoch 25/1000\n",
      "978/978 [==============================] - 1s 520us/step - loss: 6.0282 - acc: 0.0266\n",
      "Epoch 26/1000\n",
      "978/978 [==============================] - 0s 447us/step - loss: 6.0217 - acc: 0.0286\n",
      "Epoch 27/1000\n",
      "978/978 [==============================] - 0s 469us/step - loss: 6.0157 - acc: 0.0256\n",
      "Epoch 28/1000\n",
      "978/978 [==============================] - 0s 471us/step - loss: 6.0099 - acc: 0.0245\n",
      "Epoch 29/1000\n",
      "978/978 [==============================] - 0s 466us/step - loss: 6.0044 - acc: 0.0245\n",
      "Epoch 30/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 5.9989 - acc: 0.0266\n",
      "Epoch 31/1000\n",
      "978/978 [==============================] - 0s 475us/step - loss: 5.9940 - acc: 0.0286\n",
      "Epoch 32/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 5.9890 - acc: 0.0256\n",
      "Epoch 33/1000\n",
      "978/978 [==============================] - 0s 483us/step - loss: 5.9842 - acc: 0.0276\n",
      "Epoch 34/1000\n",
      "978/978 [==============================] - 0s 484us/step - loss: 5.9795 - acc: 0.0266\n",
      "Epoch 35/1000\n",
      "978/978 [==============================] - 1s 513us/step - loss: 5.9750 - acc: 0.0276\n",
      "Epoch 36/1000\n",
      "978/978 [==============================] - 0s 478us/step - loss: 5.9706 - acc: 0.0266 0s - loss: 5.9898 - acc: \n",
      "Epoch 37/1000\n",
      "978/978 [==============================] - 0s 457us/step - loss: 5.9665 - acc: 0.0276\n",
      "Epoch 38/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 5.9622 - acc: 0.0266\n",
      "Epoch 39/1000\n",
      "978/978 [==============================] - 0s 487us/step - loss: 5.9580 - acc: 0.0286\n",
      "Epoch 40/1000\n",
      "978/978 [==============================] - 0s 500us/step - loss: 5.9541 - acc: 0.0286\n",
      "Epoch 41/1000\n",
      "978/978 [==============================] - 0s 496us/step - loss: 5.9502 - acc: 0.0276\n",
      "Epoch 42/1000\n",
      "978/978 [==============================] - 0s 483us/step - loss: 5.9464 - acc: 0.0256\n",
      "Epoch 43/1000\n",
      "978/978 [==============================] - 0s 473us/step - loss: 5.9425 - acc: 0.0256\n",
      "Epoch 44/1000\n",
      "978/978 [==============================] - 0s 490us/step - loss: 5.9389 - acc: 0.0276\n",
      "Epoch 45/1000\n",
      "978/978 [==============================] - 0s 476us/step - loss: 5.9353 - acc: 0.0266\n",
      "Epoch 46/1000\n",
      "978/978 [==============================] - 0s 481us/step - loss: 5.9318 - acc: 0.0286\n",
      "Epoch 47/1000\n",
      "978/978 [==============================] - 0s 492us/step - loss: 5.9285 - acc: 0.0266\n",
      "Epoch 48/1000\n",
      "978/978 [==============================] - 0s 474us/step - loss: 5.9249 - acc: 0.0276\n",
      "Epoch 49/1000\n",
      "978/978 [==============================] - 0s 474us/step - loss: 5.9216 - acc: 0.0276\n",
      "Epoch 50/1000\n",
      "978/978 [==============================] - 1s 514us/step - loss: 5.9182 - acc: 0.0266\n",
      "Epoch 51/1000\n",
      "978/978 [==============================] - 0s 457us/step - loss: 5.9149 - acc: 0.0256\n",
      "Epoch 52/1000\n",
      "978/978 [==============================] - 0s 468us/step - loss: 5.9118 - acc: 0.0276\n",
      "Epoch 53/1000\n",
      "978/978 [==============================] - 0s 468us/step - loss: 5.9084 - acc: 0.0266\n",
      "Epoch 54/1000\n",
      "978/978 [==============================] - 0s 457us/step - loss: 5.9054 - acc: 0.0276\n",
      "Epoch 55/1000\n",
      "978/978 [==============================] - 0s 429us/step - loss: 5.9023 - acc: 0.0286\n",
      "Epoch 56/1000\n",
      "978/978 [==============================] - 0s 447us/step - loss: 5.8990 - acc: 0.0286\n",
      "Epoch 57/1000\n",
      "978/978 [==============================] - 0s 486us/step - loss: 5.8960 - acc: 0.0276\n",
      "Epoch 58/1000\n",
      "978/978 [==============================] - 1s 517us/step - loss: 5.8928 - acc: 0.0297\n",
      "Epoch 59/1000\n",
      "978/978 [==============================] - 0s 456us/step - loss: 5.8897 - acc: 0.0307\n",
      "Epoch 60/1000\n",
      "978/978 [==============================] - 0s 489us/step - loss: 5.8866 - acc: 0.0266\n",
      "Epoch 61/1000\n",
      "978/978 [==============================] - 0s 485us/step - loss: 5.8836 - acc: 0.0297\n",
      "Epoch 62/1000\n",
      "978/978 [==============================] - 0s 482us/step - loss: 5.8807 - acc: 0.0276\n",
      "Epoch 63/1000\n",
      "978/978 [==============================] - 0s 478us/step - loss: 5.8778 - acc: 0.0286\n",
      "Epoch 64/1000\n",
      "978/978 [==============================] - 0s 467us/step - loss: 5.8748 - acc: 0.0276\n",
      "Epoch 65/1000\n",
      "978/978 [==============================] - 0s 483us/step - loss: 5.8719 - acc: 0.0317\n",
      "Epoch 66/1000\n",
      "978/978 [==============================] - 0s 458us/step - loss: 5.8688 - acc: 0.0276\n",
      "Epoch 67/1000\n",
      "978/978 [==============================] - 0s 464us/step - loss: 5.8660 - acc: 0.0276\n",
      "Epoch 68/1000\n",
      "978/978 [==============================] - 0s 480us/step - loss: 5.8629 - acc: 0.0286\n",
      "Epoch 69/1000\n",
      "978/978 [==============================] - 0s 459us/step - loss: 5.8602 - acc: 0.0286\n",
      "Epoch 70/1000\n",
      "978/978 [==============================] - 0s 468us/step - loss: 5.8572 - acc: 0.0286\n",
      "Epoch 71/1000\n",
      "978/978 [==============================] - 0s 448us/step - loss: 5.8542 - acc: 0.0276\n",
      "Epoch 72/1000\n",
      "978/978 [==============================] - 0s 479us/step - loss: 5.8511 - acc: 0.0266\n",
      "Epoch 73/1000\n",
      "978/978 [==============================] - 0s 471us/step - loss: 5.8484 - acc: 0.0276\n",
      "Epoch 74/1000\n",
      "978/978 [==============================] - 0s 471us/step - loss: 5.8455 - acc: 0.0286\n",
      "Epoch 75/1000\n",
      "978/978 [==============================] - 0s 461us/step - loss: 5.8424 - acc: 0.0286\n",
      "Epoch 76/1000\n",
      "978/978 [==============================] - 0s 467us/step - loss: 5.8397 - acc: 0.0276\n",
      "Epoch 77/1000\n",
      "978/978 [==============================] - 0s 463us/step - loss: 5.8366 - acc: 0.0276\n",
      "Epoch 78/1000\n",
      "978/978 [==============================] - 0s 481us/step - loss: 5.8338 - acc: 0.0297\n",
      "Epoch 79/1000\n",
      "978/978 [==============================] - 0s 460us/step - loss: 5.8309 - acc: 0.0286\n",
      "Epoch 80/1000\n",
      "978/978 [==============================] - 0s 466us/step - loss: 5.8280 - acc: 0.0276\n",
      "Epoch 81/1000\n",
      "978/978 [==============================] - 0s 450us/step - loss: 5.8250 - acc: 0.0307\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 967/1000\n",
      "978/978 [==============================] - 0s 435us/step - loss: 3.1820 - acc: 0.3742\n",
      "Epoch 968/1000\n",
      "978/978 [==============================] - 0s 426us/step - loss: 3.1797 - acc: 0.3783\n",
      "Epoch 969/1000\n",
      "978/978 [==============================] - 0s 458us/step - loss: 3.1775 - acc: 0.3793\n",
      "Epoch 970/1000\n",
      "978/978 [==============================] - 0s 429us/step - loss: 3.1754 - acc: 0.3804\n",
      "Epoch 971/1000\n",
      "978/978 [==============================] - 0s 435us/step - loss: 3.1734 - acc: 0.3814\n",
      "Epoch 972/1000\n",
      "978/978 [==============================] - 0s 432us/step - loss: 3.1711 - acc: 0.3824\n",
      "Epoch 973/1000\n",
      "978/978 [==============================] - 0s 431us/step - loss: 3.1690 - acc: 0.3783\n",
      "Epoch 974/1000\n",
      "978/978 [==============================] - 0s 436us/step - loss: 3.1669 - acc: 0.3793\n",
      "Epoch 975/1000\n",
      "978/978 [==============================] - 0s 424us/step - loss: 3.1647 - acc: 0.3855\n",
      "Epoch 976/1000\n",
      "978/978 [==============================] - 0s 437us/step - loss: 3.1625 - acc: 0.3793\n",
      "Epoch 977/1000\n",
      "978/978 [==============================] - 0s 437us/step - loss: 3.1605 - acc: 0.3865\n",
      "Epoch 978/1000\n",
      "978/978 [==============================] - 0s 426us/step - loss: 3.1582 - acc: 0.3834\n",
      "Epoch 979/1000\n",
      "978/978 [==============================] - 0s 438us/step - loss: 3.1561 - acc: 0.3834\n",
      "Epoch 980/1000\n",
      "978/978 [==============================] - 0s 440us/step - loss: 3.1539 - acc: 0.3916\n",
      "Epoch 981/1000\n",
      "978/978 [==============================] - 0s 467us/step - loss: 3.1517 - acc: 0.3926\n",
      "Epoch 982/1000\n",
      "978/978 [==============================] - 0s 427us/step - loss: 3.1495 - acc: 0.3896\n",
      "Epoch 983/1000\n",
      "978/978 [==============================] - 0s 436us/step - loss: 3.1474 - acc: 0.3916\n",
      "Epoch 984/1000\n",
      "978/978 [==============================] - 0s 459us/step - loss: 3.1454 - acc: 0.3926\n",
      "Epoch 985/1000\n",
      "978/978 [==============================] - 0s 424us/step - loss: 3.1431 - acc: 0.3906\n",
      "Epoch 986/1000\n",
      "978/978 [==============================] - 0s 440us/step - loss: 3.1409 - acc: 0.3978\n",
      "Epoch 987/1000\n",
      "978/978 [==============================] - 0s 433us/step - loss: 3.1389 - acc: 0.3998\n",
      "Epoch 988/1000\n",
      "978/978 [==============================] - 0s 431us/step - loss: 3.1368 - acc: 0.3947\n",
      "Epoch 989/1000\n",
      "978/978 [==============================] - 0s 434us/step - loss: 3.1345 - acc: 0.3947\n",
      "Epoch 990/1000\n",
      "978/978 [==============================] - 0s 427us/step - loss: 3.1323 - acc: 0.3967\n",
      "Epoch 991/1000\n",
      "978/978 [==============================] - 0s 440us/step - loss: 3.1303 - acc: 0.3967\n",
      "Epoch 992/1000\n",
      "978/978 [==============================] - 0s 438us/step - loss: 3.1281 - acc: 0.3916\n",
      "Epoch 993/1000\n",
      "978/978 [==============================] - 0s 456us/step - loss: 3.1258 - acc: 0.3957\n",
      "Epoch 994/1000\n",
      "978/978 [==============================] - 0s 447us/step - loss: 3.1237 - acc: 0.4029\n",
      "Epoch 995/1000\n",
      "978/978 [==============================] - 0s 439us/step - loss: 3.1219 - acc: 0.4008\n",
      "Epoch 996/1000\n",
      "978/978 [==============================] - 0s 445us/step - loss: 3.1195 - acc: 0.4039\n",
      "Epoch 997/1000\n",
      "978/978 [==============================] - 0s 443us/step - loss: 3.1173 - acc: 0.3988\n",
      "Epoch 998/1000\n",
      "978/978 [==============================] - 0s 441us/step - loss: 3.1153 - acc: 0.4018\n",
      "Epoch 999/1000\n",
      "978/978 [==============================] - 0s 421us/step - loss: 3.1131 - acc: 0.4049\n",
      "Epoch 1000/1000\n",
      "978/978 [==============================] - 0s 435us/step - loss: 3.1111 - acc: 0.3998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x766eeb4be0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_data, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre = model.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['굳다', 'ㅋ', 'GDNTOPCLASSINTHECLUB']===>252 뭐\n",
      "['ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐']===>445 음악\n",
      "['GDNTOPCLASSINTHECLUB', '뭐', '이']===>595 하다\n",
      "['뭐', '이', '평점']===>185 들다\n",
      "['이', '평점', '들']===>178 되다\n",
      "['평점', '들', '나쁘다']===>67 결국\n",
      "['들', '나쁘다', '않다']===>1 10\n",
      "['나쁘다', '않다', '10']===>496 점\n",
      "['않다', '10', '점']===>539 짜다\n",
      "['10', '점', '짜다']===>493 적\n",
      "['점', '짜다', '리']===>237 몇번\n",
      "['짜다', '리', '더']===>166 더욱\n",
      "['리', '더', '더욱']===>219 막판\n",
      "['더', '더욱', '아니다']===>528 지루하다\n",
      "['더욱', '아니다', '지루하다']===>576 톰\n",
      "['아니다', '지루하다', '않다']===>576 톰\n",
      "['지루하다', '않다', '완전']===>511 좀\n",
      "['않다', '완전', '막장']===>595 하다\n",
      "['완전', '막장', '임']===>450 이\n",
      "['막장', '임', '돈']===>515 주다\n",
      "['임', '돈', '주다']===>450 이\n",
      "['돈', '주다', '보기']===>10 3\n",
      "['주다', '보기', '3']===>20 d\n",
      "['보기', '3', 'D']===>220 만\n",
      "['3', 'D', '만']===>512 좋다\n",
      "['D', '만', '아니다']===>294 비교\n",
      "['만', '아니다', '별']===>185 들다\n",
      "['아니다', '별', '다섯']===>143 놓다\n",
      "['별', '다섯', '개']===>416 영화\n",
      "['다섯', '개', '주다']===>450 이\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(tokenized_words[i: i+3], end='===>')\n",
    "    print(pre[i].argmax(), end=' ')\n",
    "    print(vect.get_feature_names()[pre[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
