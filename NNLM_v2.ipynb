{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  굳 ㅋ\n",
      "1                                 GDNTOPCLASSINTHECLUB\n",
      "2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
      "3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
      "4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
      "5                                   음악이 주가 된, 최고의 음악영화\n",
      "6                                              진정한 쓰레기\n",
      "7             마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
      "8    갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...\n",
      "9       이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n",
      "Name: document, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# 영화 후기 데이터 -> NNLM 모델 적용 \n",
    "#######\n",
    "\n",
    "### 데이터 읽기\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "df_train = pd.read_csv('100 data/ratings_test.txt', delimiter='\\t', keep_default_na=False)\n",
    "print(df_train['document'][:10])\n",
    "\n",
    "# Data는 100개만 사용\n",
    "text = df_train['document'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['괜히', '굉장하다', '교훈', '구성', '군', '굳다', '굿굿', '궁금', '귀엽다', '그']\n",
      "622\n"
     ]
    }
   ],
   "source": [
    "### 형태소 분석 및 one-hot encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "twitter_tag = Okt()\n",
    "\n",
    "# 형태소 분석 방법(조사, 어미 등 제외)\n",
    "def twitter_tokenizer_part(text):\n",
    "    lst = []\n",
    "    for tpl in twitter_tag.pos(text, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            lst.append(tpl[0])\n",
    "    if len(lst) != 0: \n",
    "        return lst\n",
    "    else:\n",
    "        return [\"\"]\n",
    "    \n",
    "# CountVectorizer 객체 형성 -> one-hot encoding\n",
    "vect = CountVectorizer(tokenizer=twitter_tokenizer_part).fit(text)\n",
    "print(vect.get_feature_names()[80:90])\n",
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['점', '짜다', '리', '더', '더욱', '아니다', '지루하다', '않다', '완전', '막장']\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "# 입력할 후기를 토큰화함\n",
    "tokenized_words = []\n",
    "for line in text:\n",
    "    for tpl in twitter_tag.pos(line, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            tokenized_words.append(tpl[0])\n",
    "\n",
    "print(tokenized_words[10:20])\n",
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-1, n-2, n-3번째 단어를 통해 n번째 단어를 예측하는 학습 데이터 생성\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(len(tokenized_words)-3):\n",
    "    x_train.append(vect.transform([tokenized_words[i],tokenized_words[i+1],tokenized_words[i+2]]).toarray())\n",
    "    y_train.append(vect.transform([tokenized_words[i+3]]).toarray())\n",
    "    \n",
    "# 데이터 예시\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 리스트를 array로 변형\n",
    "import numpy as np\n",
    "x_train = np.array(x_train).reshape(978, 3, 622)\n",
    "y_train = np.array(y_train).reshape(978, 622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### 모델 생성 \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 622)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3, 2)              1246      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 622)               40430     \n",
      "=================================================================\n",
      "Total params: 42,124\n",
      "Trainable params: 42,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 방식1: one-hot encoding 형태로 입력\n",
    "x = Input([3,len(y_train[0])])\n",
    "x2 = Dense(2)(x)\n",
    "# 방식2: index 형태로 입력 \n",
    "# x = Input((3,))\n",
    "# x2 = Embedding(y_train[0], 2)(x)\n",
    "x3 = Flatten()(x2)\n",
    "h = Dense(64, activation='tanh')(x3)\n",
    "y = Dense(len(y_train[0]), activation='softmax')(h)\n",
    "model = Model(x,y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1) # default = 0.01\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "978/978 [==============================] - 1s 648us/step - loss: 6.3271 - acc: 0.0082\n",
      "Epoch 2/1000\n",
      "978/978 [==============================] - 0s 280us/step - loss: 6.3240 - acc: 0.0123\n",
      "Epoch 3/1000\n",
      "978/978 [==============================] - 0s 287us/step - loss: 6.3209 - acc: 0.0204\n",
      "Epoch 4/1000\n",
      "978/978 [==============================] - 0s 276us/step - loss: 6.3178 - acc: 0.0204\n",
      "Epoch 5/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 6.3147 - acc: 0.0235\n",
      "Epoch 6/1000\n",
      "978/978 [==============================] - 0s 288us/step - loss: 6.3116 - acc: 0.0174\n",
      "Epoch 7/1000\n",
      "978/978 [==============================] - 0s 260us/step - loss: 6.3085 - acc: 0.0143\n",
      "Epoch 8/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 6.3052 - acc: 0.0215\n",
      "Epoch 9/1000\n",
      "978/978 [==============================] - 0s 261us/step - loss: 6.3020 - acc: 0.0153\n",
      "Epoch 10/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 6.2987 - acc: 0.0174\n",
      "Epoch 11/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 6.2954 - acc: 0.0245\n",
      "Epoch 12/1000\n",
      "978/978 [==============================] - 0s 270us/step - loss: 6.2919 - acc: 0.0225\n",
      "Epoch 13/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 6.2883 - acc: 0.0194\n",
      "Epoch 14/1000\n",
      "978/978 [==============================] - 0s 265us/step - loss: 6.2846 - acc: 0.0215\n",
      "Epoch 15/1000\n",
      "978/978 [==============================] - 0s 276us/step - loss: 6.2807 - acc: 0.0225\n",
      "Epoch 16/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 6.2766 - acc: 0.0245\n",
      "Epoch 17/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 6.2723 - acc: 0.0225\n",
      "Epoch 18/1000\n",
      "978/978 [==============================] - 0s 283us/step - loss: 6.2676 - acc: 0.0235\n",
      "Epoch 19/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 6.2625 - acc: 0.0143\n",
      "Epoch 20/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 6.2570 - acc: 0.0245\n",
      "Epoch 21/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 6.2509 - acc: 0.0245\n",
      "Epoch 22/1000\n",
      "978/978 [==============================] - 0s 300us/step - loss: 6.2441 - acc: 0.0174\n",
      "Epoch 23/1000\n",
      "978/978 [==============================] - 0s 298us/step - loss: 6.2364 - acc: 0.0235\n",
      "Epoch 24/1000\n",
      "978/978 [==============================] - 0s 315us/step - loss: 6.2275 - acc: 0.0245\n",
      "Epoch 25/1000\n",
      "978/978 [==============================] - 0s 301us/step - loss: 6.2175 - acc: 0.0245\n",
      "Epoch 26/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 6.2063 - acc: 0.0245\n",
      "Epoch 27/1000\n",
      "978/978 [==============================] - 0s 262us/step - loss: 6.1935 - acc: 0.0245\n",
      "Epoch 28/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 6.1793 - acc: 0.0245\n",
      "Epoch 29/1000\n",
      "978/978 [==============================] - 0s 363us/step - loss: 6.1635 - acc: 0.0245\n",
      "Epoch 30/1000\n",
      "978/978 [==============================] - 0s 297us/step - loss: 6.1467 - acc: 0.0245\n",
      "Epoch 31/1000\n",
      "978/978 [==============================] - 0s 248us/step - loss: 6.1293 - acc: 0.0245\n",
      "Epoch 32/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 6.1130 - acc: 0.0245\n",
      "Epoch 33/1000\n",
      "978/978 [==============================] - 0s 366us/step - loss: 6.0983 - acc: 0.0245\n",
      "Epoch 34/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 6.0852 - acc: 0.0245\n",
      "Epoch 35/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 6.0746 - acc: 0.0245\n",
      "Epoch 36/1000\n",
      "978/978 [==============================] - 0s 239us/step - loss: 6.0654 - acc: 0.0245\n",
      "Epoch 37/1000\n",
      "978/978 [==============================] - 0s 281us/step - loss: 6.0574 - acc: 0.0245\n",
      "Epoch 38/1000\n",
      "978/978 [==============================] - 0s 311us/step - loss: 6.0504 - acc: 0.0245\n",
      "Epoch 39/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 6.0442 - acc: 0.0235\n",
      "Epoch 40/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 6.0379 - acc: 0.0245\n",
      "Epoch 41/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 6.0330 - acc: 0.0245\n",
      "Epoch 42/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 6.0286 - acc: 0.0235\n",
      "Epoch 43/1000\n",
      "978/978 [==============================] - 0s 250us/step - loss: 6.0238 - acc: 0.0225\n",
      "Epoch 44/1000\n",
      "978/978 [==============================] - 0s 249us/step - loss: 6.0201 - acc: 0.0245\n",
      "Epoch 45/1000\n",
      "978/978 [==============================] - 0s 293us/step - loss: 6.0168 - acc: 0.0235\n",
      "Epoch 46/1000\n",
      "978/978 [==============================] - 0s 351us/step - loss: 6.0136 - acc: 0.0245\n",
      "Epoch 47/1000\n",
      "978/978 [==============================] - 0s 278us/step - loss: 6.0111 - acc: 0.0245\n",
      "Epoch 48/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 6.0087 - acc: 0.0245\n",
      "Epoch 49/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 6.0065 - acc: 0.0184\n",
      "Epoch 50/1000\n",
      "978/978 [==============================] - 0s 252us/step - loss: 6.0047 - acc: 0.0215\n",
      "Epoch 51/1000\n",
      "978/978 [==============================] - 0s 252us/step - loss: 6.0032 - acc: 0.0245\n",
      "Epoch 52/1000\n",
      "978/978 [==============================] - 0s 245us/step - loss: 6.0012 - acc: 0.0184\n",
      "Epoch 53/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 5.9994 - acc: 0.0215\n",
      "Epoch 54/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 5.9985 - acc: 0.0256\n",
      "Epoch 55/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 5.9967 - acc: 0.0245\n",
      "Epoch 56/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 5.9955 - acc: 0.0245\n",
      "Epoch 57/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 5.9951 - acc: 0.0174\n",
      "Epoch 58/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9935 - acc: 0.0245\n",
      "Epoch 59/1000\n",
      "978/978 [==============================] - 0s 249us/step - loss: 5.9925 - acc: 0.0174\n",
      "Epoch 60/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 5.9919 - acc: 0.0245\n",
      "Epoch 61/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 5.9907 - acc: 0.0225\n",
      "Epoch 62/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 5.9899 - acc: 0.0245\n",
      "Epoch 63/1000\n",
      "978/978 [==============================] - 0s 248us/step - loss: 5.9886 - acc: 0.0164\n",
      "Epoch 64/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 5.9883 - acc: 0.0225\n",
      "Epoch 65/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9879 - acc: 0.0235\n",
      "Epoch 66/1000\n",
      "978/978 [==============================] - 0s 237us/step - loss: 5.9870 - acc: 0.0266\n",
      "Epoch 67/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9864 - acc: 0.0204\n",
      "Epoch 68/1000\n",
      "978/978 [==============================] - 0s 235us/step - loss: 5.9861 - acc: 0.0245\n",
      "Epoch 69/1000\n",
      "978/978 [==============================] - 0s 247us/step - loss: 5.9854 - acc: 0.0215\n",
      "Epoch 70/1000\n",
      "978/978 [==============================] - 0s 245us/step - loss: 5.9847 - acc: 0.0245\n",
      "Epoch 71/1000\n",
      "978/978 [==============================] - 0s 241us/step - loss: 5.9843 - acc: 0.0204\n",
      "Epoch 72/1000\n",
      "978/978 [==============================] - 0s 243us/step - loss: 5.9834 - acc: 0.0266\n",
      "Epoch 73/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 5.9834 - acc: 0.0184\n",
      "Epoch 74/1000\n",
      "978/978 [==============================] - 0s 276us/step - loss: 5.9826 - acc: 0.0235\n",
      "Epoch 75/1000\n",
      "978/978 [==============================] - 0s 249us/step - loss: 5.9822 - acc: 0.0215\n",
      "Epoch 76/1000\n",
      "978/978 [==============================] - 0s 247us/step - loss: 5.9826 - acc: 0.0184\n",
      "Epoch 77/1000\n",
      "978/978 [==============================] - 0s 241us/step - loss: 5.9814 - acc: 0.0194\n",
      "Epoch 78/1000\n",
      "978/978 [==============================] - 0s 239us/step - loss: 5.9819 - acc: 0.0174\n",
      "Epoch 79/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9813 - acc: 0.0235\n",
      "Epoch 80/1000\n",
      "978/978 [==============================] - 0s 236us/step - loss: 5.9803 - acc: 0.0245\n",
      "Epoch 81/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9802 - acc: 0.0235\n",
      "Epoch 82/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 5.9799 - acc: 0.0215\n",
      "Epoch 83/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978/978 [==============================] - 0s 364us/step - loss: 1.6998 - acc: 0.7423\n",
      "Epoch 889/1000\n",
      "978/978 [==============================] - 0s 308us/step - loss: 1.6960 - acc: 0.7434\n",
      "Epoch 890/1000\n",
      "978/978 [==============================] - 0s 281us/step - loss: 1.6924 - acc: 0.7444\n",
      "Epoch 891/1000\n",
      "978/978 [==============================] - 0s 350us/step - loss: 1.6871 - acc: 0.7423 0s - loss: 1.7252 - acc: 0.73\n",
      "Epoch 892/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 1.6818 - acc: 0.7434\n",
      "Epoch 893/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 1.6793 - acc: 0.7485\n",
      "Epoch 894/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 1.6734 - acc: 0.7474\n",
      "Epoch 895/1000\n",
      "978/978 [==============================] - 0s 285us/step - loss: 1.6719 - acc: 0.7495\n",
      "Epoch 896/1000\n",
      "978/978 [==============================] - 0s 259us/step - loss: 1.6675 - acc: 0.7505\n",
      "Epoch 897/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 1.6625 - acc: 0.7536\n",
      "Epoch 898/1000\n",
      "978/978 [==============================] - 0s 272us/step - loss: 1.6586 - acc: 0.7546\n",
      "Epoch 899/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 1.6541 - acc: 0.7566\n",
      "Epoch 900/1000\n",
      "978/978 [==============================] - 0s 281us/step - loss: 1.6484 - acc: 0.7607\n",
      "Epoch 901/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 1.6483 - acc: 0.7648\n",
      "Epoch 902/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.6435 - acc: 0.7566\n",
      "Epoch 903/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 1.6368 - acc: 0.7566\n",
      "Epoch 904/1000\n",
      "978/978 [==============================] - 0s 260us/step - loss: 1.6307 - acc: 0.7618\n",
      "Epoch 905/1000\n",
      "978/978 [==============================] - 0s 278us/step - loss: 1.6278 - acc: 0.7648\n",
      "Epoch 906/1000\n",
      "978/978 [==============================] - 0s 268us/step - loss: 1.6233 - acc: 0.7669\n",
      "Epoch 907/1000\n",
      "978/978 [==============================] - 0s 272us/step - loss: 1.6241 - acc: 0.7587\n",
      "Epoch 908/1000\n",
      "978/978 [==============================] - 0s 258us/step - loss: 1.6149 - acc: 0.7720\n",
      "Epoch 909/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 1.6134 - acc: 0.7658 0s - loss: 1.5910 - acc: 0.77\n",
      "Epoch 910/1000\n",
      "978/978 [==============================] - 0s 260us/step - loss: 1.6074 - acc: 0.7699\n",
      "Epoch 911/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 1.6041 - acc: 0.7669\n",
      "Epoch 912/1000\n",
      "978/978 [==============================] - 0s 269us/step - loss: 1.5982 - acc: 0.7751\n",
      "Epoch 913/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 1.5956 - acc: 0.7781\n",
      "Epoch 914/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 1.5919 - acc: 0.7740\n",
      "Epoch 915/1000\n",
      "978/978 [==============================] - 0s 257us/step - loss: 1.5858 - acc: 0.7751\n",
      "Epoch 916/1000\n",
      "978/978 [==============================] - 0s 259us/step - loss: 1.5826 - acc: 0.7802\n",
      "Epoch 917/1000\n",
      "978/978 [==============================] - 0s 245us/step - loss: 1.5821 - acc: 0.7791\n",
      "Epoch 918/1000\n",
      "978/978 [==============================] - 0s 258us/step - loss: 1.5731 - acc: 0.7812 0s - loss: 1.5716 - acc: 0.775\n",
      "Epoch 919/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.5727 - acc: 0.7853\n",
      "Epoch 920/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 1.5671 - acc: 0.7812\n",
      "Epoch 921/1000\n",
      "978/978 [==============================] - 0s 268us/step - loss: 1.5639 - acc: 0.7843\n",
      "Epoch 922/1000\n",
      "978/978 [==============================] - 0s 265us/step - loss: 1.5630 - acc: 0.7843\n",
      "Epoch 923/1000\n",
      "978/978 [==============================] - 0s 261us/step - loss: 1.5552 - acc: 0.7832\n",
      "Epoch 924/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.5500 - acc: 0.7863\n",
      "Epoch 925/1000\n",
      "978/978 [==============================] - 0s 257us/step - loss: 1.5464 - acc: 0.7883\n",
      "Epoch 926/1000\n",
      "978/978 [==============================] - 0s 255us/step - loss: 1.5425 - acc: 0.7863\n",
      "Epoch 927/1000\n",
      "978/978 [==============================] - 0s 258us/step - loss: 1.5397 - acc: 0.7914\n",
      "Epoch 928/1000\n",
      "978/978 [==============================] - 0s 252us/step - loss: 1.5341 - acc: 0.7965\n",
      "Epoch 929/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 1.5327 - acc: 0.7975\n",
      "Epoch 930/1000\n",
      "978/978 [==============================] - 0s 250us/step - loss: 1.5264 - acc: 0.7986\n",
      "Epoch 931/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.5240 - acc: 0.7955\n",
      "Epoch 932/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.5185 - acc: 0.7996\n",
      "Epoch 933/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.5142 - acc: 0.7945\n",
      "Epoch 934/1000\n",
      "978/978 [==============================] - 0s 272us/step - loss: 1.5128 - acc: 0.7975\n",
      "Epoch 935/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 1.5083 - acc: 0.7975\n",
      "Epoch 936/1000\n",
      "978/978 [==============================] - 0s 354us/step - loss: 1.5040 - acc: 0.7975\n",
      "Epoch 937/1000\n",
      "978/978 [==============================] - 0s 306us/step - loss: 1.5002 - acc: 0.8006\n",
      "Epoch 938/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 1.4955 - acc: 0.7986\n",
      "Epoch 939/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 1.4923 - acc: 0.8057\n",
      "Epoch 940/1000\n",
      "978/978 [==============================] - 0s 237us/step - loss: 1.4899 - acc: 0.8108\n",
      "Epoch 941/1000\n",
      "978/978 [==============================] - 0s 252us/step - loss: 1.4837 - acc: 0.8088\n",
      "Epoch 942/1000\n",
      "978/978 [==============================] - 0s 245us/step - loss: 1.4811 - acc: 0.8078\n",
      "Epoch 943/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.4762 - acc: 0.8088\n",
      "Epoch 944/1000\n",
      "978/978 [==============================] - 0s 247us/step - loss: 1.4712 - acc: 0.8149\n",
      "Epoch 945/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.4702 - acc: 0.8098\n",
      "Epoch 946/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 1.4641 - acc: 0.8078\n",
      "Epoch 947/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 1.4614 - acc: 0.8119\n",
      "Epoch 948/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.4575 - acc: 0.8098\n",
      "Epoch 949/1000\n",
      "978/978 [==============================] - 0s 246us/step - loss: 1.4531 - acc: 0.8098\n",
      "Epoch 950/1000\n",
      "978/978 [==============================] - 0s 239us/step - loss: 1.4540 - acc: 0.8119\n",
      "Epoch 951/1000\n",
      "978/978 [==============================] - 0s 242us/step - loss: 1.4453 - acc: 0.8200\n",
      "Epoch 952/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 1.4430 - acc: 0.8139\n",
      "Epoch 953/1000\n",
      "978/978 [==============================] - 0s 239us/step - loss: 1.4380 - acc: 0.8211\n",
      "Epoch 954/1000\n",
      "978/978 [==============================] - 0s 265us/step - loss: 1.4352 - acc: 0.8252\n",
      "Epoch 955/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 1.4316 - acc: 0.8231\n",
      "Epoch 956/1000\n",
      "978/978 [==============================] - 0s 240us/step - loss: 1.4280 - acc: 0.8211\n",
      "Epoch 957/1000\n",
      "978/978 [==============================] - 0s 252us/step - loss: 1.4226 - acc: 0.8313\n",
      "Epoch 958/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 1.4237 - acc: 0.8252\n",
      "Epoch 959/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 1.4157 - acc: 0.8252\n",
      "Epoch 960/1000\n",
      "978/978 [==============================] - 0s 238us/step - loss: 1.4129 - acc: 0.8333\n",
      "Epoch 961/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 1.4092 - acc: 0.8303\n",
      "Epoch 962/1000\n",
      "978/978 [==============================] - 0s 243us/step - loss: 1.4050 - acc: 0.8344\n",
      "Epoch 963/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 1.4029 - acc: 0.8272\n",
      "Epoch 964/1000\n",
      "978/978 [==============================] - 0s 237us/step - loss: 1.4002 - acc: 0.8354\n",
      "Epoch 965/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 1.3951 - acc: 0.8344\n",
      "Epoch 966/1000\n",
      "978/978 [==============================] - 0s 244us/step - loss: 1.3897 - acc: 0.8384\n",
      "Epoch 967/1000\n",
      "978/978 [==============================] - 0s 250us/step - loss: 1.3880 - acc: 0.8405\n",
      "Epoch 968/1000\n",
      "978/978 [==============================] - 0s 248us/step - loss: 1.3832 - acc: 0.8425\n",
      "Epoch 969/1000\n",
      "978/978 [==============================] - 0s 245us/step - loss: 1.3809 - acc: 0.8446\n",
      "Epoch 970/1000\n",
      "978/978 [==============================] - 0s 284us/step - loss: 1.3760 - acc: 0.8436\n",
      "Epoch 971/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 1.3728 - acc: 0.8456\n",
      "Epoch 972/1000\n",
      "978/978 [==============================] - 0s 281us/step - loss: 1.3698 - acc: 0.8374\n",
      "Epoch 973/1000\n",
      "978/978 [==============================] - 0s 282us/step - loss: 1.3662 - acc: 0.8425\n",
      "Epoch 974/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 1.3628 - acc: 0.8466\n",
      "Epoch 975/1000\n",
      "978/978 [==============================] - 0s 273us/step - loss: 1.3592 - acc: 0.8456\n",
      "Epoch 976/1000\n",
      "978/978 [==============================] - 0s 276us/step - loss: 1.3545 - acc: 0.8466\n",
      "Epoch 977/1000\n",
      "978/978 [==============================] - 0s 271us/step - loss: 1.3510 - acc: 0.8487\n",
      "Epoch 978/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 1.3483 - acc: 0.8497\n",
      "Epoch 979/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 1.3479 - acc: 0.8405\n",
      "Epoch 980/1000\n",
      "978/978 [==============================] - 0s 263us/step - loss: 1.3397 - acc: 0.8517\n",
      "Epoch 981/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 1.3410 - acc: 0.8436\n",
      "Epoch 982/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 1.3349 - acc: 0.8558\n",
      "Epoch 983/1000\n",
      "978/978 [==============================] - 0s 271us/step - loss: 1.3303 - acc: 0.8548\n",
      "Epoch 984/1000\n",
      "978/978 [==============================] - 0s 265us/step - loss: 1.3273 - acc: 0.8558\n",
      "Epoch 985/1000\n",
      "978/978 [==============================] - 0s 266us/step - loss: 1.3228 - acc: 0.8579\n",
      "Epoch 986/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 1.3205 - acc: 0.8497\n",
      "Epoch 987/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.3164 - acc: 0.8548\n",
      "Epoch 988/1000\n",
      "978/978 [==============================] - 0s 267us/step - loss: 1.3173 - acc: 0.8558\n",
      "Epoch 989/1000\n",
      "978/978 [==============================] - 0s 271us/step - loss: 1.3106 - acc: 0.8569\n",
      "Epoch 990/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.3057 - acc: 0.8599\n",
      "Epoch 991/1000\n",
      "978/978 [==============================] - 0s 263us/step - loss: 1.3047 - acc: 0.8569\n",
      "Epoch 992/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.2997 - acc: 0.8620\n",
      "Epoch 993/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 1.2986 - acc: 0.8620\n",
      "Epoch 994/1000\n",
      "978/978 [==============================] - 0s 265us/step - loss: 1.2924 - acc: 0.8691\n",
      "Epoch 995/1000\n",
      "978/978 [==============================] - 0s 253us/step - loss: 1.2907 - acc: 0.8650\n",
      "Epoch 996/1000\n",
      "978/978 [==============================] - 0s 251us/step - loss: 1.2863 - acc: 0.8681\n",
      "Epoch 997/1000\n",
      "978/978 [==============================] - 0s 256us/step - loss: 1.2840 - acc: 0.8640\n",
      "Epoch 998/1000\n",
      "978/978 [==============================] - 0s 262us/step - loss: 1.2802 - acc: 0.8722\n",
      "Epoch 999/1000\n",
      "978/978 [==============================] - 0s 258us/step - loss: 1.2764 - acc: 0.8712\n",
      "Epoch 1000/1000\n",
      "978/978 [==============================] - 0s 250us/step - loss: 1.2746 - acc: 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x682912c898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=1000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['되다', '최고', '음악']===>416 영화\n",
      "['최고', '음악', '영화']===>533 진정하다\n",
      "['음악', '영화', '진정하다']===>352 쓰레기\n",
      "['영화', '진정하다', '쓰레기']===>216 마치\n",
      "['진정하다', '쓰레기', '마치']===>255 미국\n",
      "['쓰레기', '마치', '미국']===>379 애니\n",
      "['마치', '미국', '애니']===>578 튀어나오다\n",
      "['미국', '애니', '튀어나오다']===>595 하다\n",
      "['애니', '튀어나오다', '한']===>549 창의력\n",
      "['튀어나오다', '한', '창의력']===>398 없다\n",
      "['한', '창의력', '없다']===>202 로봇\n",
      "['창의력', '없다', '로봇']===>188 디자인\n",
      "['없다', '로봇', '디자인']===>72 고개\n",
      "['로봇', '디자인', '고개']===>500 젖다\n",
      "['디자인', '고개', '젖다']===>595 하다\n",
      "['고개', '젖다', '하다']===>47 갈수록\n",
      "['젖다', '하다', '갈수록']===>57 개판\n",
      "['하다', '갈수록', '개판']===>178 되다\n",
      "['갈수록', '개판', '되다']===>521 중국영화\n",
      "['개판', '되다', '중국영화']===>440 유치하다\n",
      "['되다', '중국영화', '유치하다']===>416 영화\n",
      "['중국영화', '유치하다', '내용']===>398 없다\n",
      "['유치하다', '내용', '없다']===>588 폼\n",
      "['내용', '없다', '폼']===>483 잡다\n",
      "['없다', '폼', '잡다']===>115 끝나다\n",
      "['폼', '잡다', '끝나다']===>185 들다\n",
      "['잡다', '끝나다', '말']===>595 하다\n",
      "['끝나다', '말', '안되다']===>244 무기\n",
      "['말', '안되다', '무기']===>440 유치하다\n",
      "['안되다', '무기', '유치하다']===>19 cg\n",
      "['무기', '유치하다', 'cg']===>128 남무\n",
      "['유치하다', 'cg', '남무']===>512 좋다\n",
      "['cg', '남무', '아']===>89 그\n",
      "['남무', '아', '그리다']===>176 동사서독\n",
      "['아', '그리다', '동사서독']===>52 같다\n",
      "['그리다', '동사서독', '같다']===>492 저\n",
      "['동사서독', '같다', '영화']===>451 이건\n",
      "['같다', '영화', '이건']===>10 3\n",
      "['영화', '이건', '3']===>206 류\n",
      "['이건', '3', '류']===>207 류작\n",
      "['3', '류', '류작']===>460 이별\n",
      "['류', '류작', '이별']===>367 아픔\n",
      "['류작', '이별', '아픔']===>181 뒤\n",
      "['이별', '아픔', '뒤']===>550 찾아오다\n",
      "['아픔', '뒤', '찾아오다']===>312 새롭다\n",
      "['뒤', '찾아오다', '새롭다']===>469 인연\n",
      "['찾아오다', '새롭다', '인연']===>106 기쁨\n",
      "['새롭다', '인연', '기쁨']===>17 but\n",
      "['인연', '기쁨', 'But']===>238 모든\n",
      "['기쁨', 'But', '모든']===>304 사람\n",
      "['But', '모든', '사람']===>92 그렇다\n",
      "['모든', '사람', '그렇다']===>374 않다\n",
      "['사람', '그렇다', '않다']===>79 괜찮다\n",
      "['그렇다', '않다', '괜찮다']===>420 오랜\n",
      "['않다', '괜찮다', '오랜']===>220 만\n",
      "['괜찮다', '오랜', '만']===>587 포켓몬스터\n"
     ]
    }
   ],
   "source": [
    "for i in range(43,99):\n",
    "    print(tokenized_words[i: i+3], end='===>')\n",
    "    print(pre[i].argmax(), end=' ')\n",
    "    print(vect.get_feature_names()[pre[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(622, 2)\n"
     ]
    }
   ],
   "source": [
    "# 워드임배딩 잘됐는지 확인 \n",
    "embedded_W = model.get_weights()[0]\n",
    "print(embedded_W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 코사인 유사도 함수 의 \n",
    "def cosine_similarity(A, B):\n",
    "    if np.sum(A*B) == 0:\n",
    "        return 0\n",
    "    return np.sum(A*B) / (np.sqrt(np.sum(A*A)) * np.sqrt(np.sum(B*B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화\n",
      "쓰레기\n",
      "포켓몬스터\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[416])\n",
    "print(vect.get_feature_names()[352])\n",
    "print(vect.get_feature_names()[587])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47847405"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화와 쓰레기의 코사인 유사도 \n",
    "A = embedded_W[416]\n",
    "B = embedded_W[352]\n",
    "cosine_similarity(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.068978064"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화와 포켓몬스터의 코사인 유사도 \n",
    "A = embedded_W[416]\n",
    "B = embedded_W[587]\n",
    "cosine_similarity(A,B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
