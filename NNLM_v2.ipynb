{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  굳 ㅋ\n",
      "1                                 GDNTOPCLASSINTHECLUB\n",
      "2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
      "3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
      "4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
      "5                                   음악이 주가 된, 최고의 음악영화\n",
      "6                                              진정한 쓰레기\n",
      "7             마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
      "8    갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...\n",
      "9       이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n",
      "Name: document, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# 영화 후기 데이터 -> NNLM 모델 적용 \n",
    "#######\n",
    "\n",
    "### 데이터 읽기\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "df_train = pd.read_csv('100 data/ratings_test.txt', delimiter='\\t', keep_default_na=False)\n",
    "print(df_train['document'][:10])\n",
    "\n",
    "# Data는 100개만 사용\n",
    "text = df_train['document'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['괜히', '굉장하다', '교훈', '구성', '군', '굳다', '굿굿', '궁금', '귀엽다', '그']\n",
      "622\n"
     ]
    }
   ],
   "source": [
    "### 형태소 분석 및 one-hot encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "twitter_tag = Okt()\n",
    "\n",
    "# 형태소 분석 방법(조사, 어미 등 제외)\n",
    "def twitter_tokenizer_part(text):\n",
    "    lst = []\n",
    "    for tpl in twitter_tag.pos(text, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            lst.append(tpl[0])\n",
    "    if len(lst) != 0: \n",
    "        return lst\n",
    "    else:\n",
    "        return [\"\"]\n",
    "    \n",
    "# CountVectorizer 객체 형성 -> one-hot encoding\n",
    "vect = CountVectorizer(tokenizer=twitter_tokenizer_part).fit(text)\n",
    "print(vect.get_feature_names()[80:90])\n",
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['점', '짜다', '리', '더', '더욱', '아니다', '지루하다', '않다', '완전', '막장']\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "# 입력할 후기를 토큰화함\n",
    "tokenized_words = []\n",
    "for line in text:\n",
    "    for tpl in twitter_tag.pos(line, stem=True):\n",
    "        if not tpl[1] in [\"Josa\", \"Eomi\", \"PreEomi\", \"Punctuation\"]:\n",
    "            tokenized_words.append(tpl[0])\n",
    "\n",
    "print(tokenized_words[10:20])\n",
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-1, n-2, n-3번째 단어를 통해 n번째 단어를 예측하는 학습 데이터 생성\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(len(tokenized_words)-3):\n",
    "    x_train.append(vect.transform([tokenized_words[i],tokenized_words[i+1],tokenized_words[i+2]]).toarray())\n",
    "    y_train.append(vect.transform([tokenized_words[i+3]]).toarray())\n",
    "    \n",
    "# 데이터 예시\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 array로 변형\n",
    "import numpy as np\n",
    "x_train = np.array(x_train).reshape(978, 3, 622)\n",
    "y_train = np.array(y_train).reshape(978, 622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 생성 \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3, 622)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3, 2)              1246      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 622)               40430     \n",
      "=================================================================\n",
      "Total params: 42,124\n",
      "Trainable params: 42,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input([3,len(y_train[0])])\n",
    "x2 = Dense(2)(x)\n",
    "x3 = Flatten()(x2)\n",
    "h = Dense(64, activation='tanh')(x3)\n",
    "y = Dense(len(y_train[0]), activation='softmax')(h)\n",
    "model = Model(x,y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1) # default = 0.01\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "978/978 [==============================] - 1s 722us/step - loss: 6.3270 - acc: 0.0072\n",
      "Epoch 2/1000\n",
      "978/978 [==============================] - 0s 287us/step - loss: 6.3237 - acc: 0.0276\n",
      "Epoch 3/1000\n",
      "978/978 [==============================] - 0s 294us/step - loss: 6.3204 - acc: 0.0276\n",
      "Epoch 4/1000\n",
      "978/978 [==============================] - 0s 317us/step - loss: 6.3171 - acc: 0.0256\n",
      "Epoch 5/1000\n",
      "978/978 [==============================] - 0s 301us/step - loss: 6.3137 - acc: 0.0286\n",
      "Epoch 6/1000\n",
      "978/978 [==============================] - 0s 307us/step - loss: 6.3103 - acc: 0.0297\n",
      "Epoch 7/1000\n",
      "978/978 [==============================] - 0s 311us/step - loss: 6.3069 - acc: 0.0276\n",
      "Epoch 8/1000\n",
      "978/978 [==============================] - 0s 300us/step - loss: 6.3033 - acc: 0.0276\n",
      "Epoch 9/1000\n",
      "978/978 [==============================] - 0s 327us/step - loss: 6.2997 - acc: 0.0256\n",
      "Epoch 10/1000\n",
      "978/978 [==============================] - 0s 395us/step - loss: 6.2959 - acc: 0.0276\n",
      "Epoch 11/1000\n",
      "978/978 [==============================] - 0s 372us/step - loss: 6.2919 - acc: 0.0286\n",
      "Epoch 12/1000\n",
      "978/978 [==============================] - 0s 322us/step - loss: 6.2877 - acc: 0.0286\n",
      "Epoch 13/1000\n",
      "978/978 [==============================] - 0s 293us/step - loss: 6.2831 - acc: 0.0276\n",
      "Epoch 14/1000\n",
      "978/978 [==============================] - 0s 281us/step - loss: 6.2781 - acc: 0.0286\n",
      "Epoch 15/1000\n",
      "978/978 [==============================] - 0s 285us/step - loss: 6.2726 - acc: 0.0276\n",
      "Epoch 16/1000\n",
      "978/978 [==============================] - 0s 287us/step - loss: 6.2666 - acc: 0.0266\n",
      "Epoch 17/1000\n",
      "978/978 [==============================] - 0s 285us/step - loss: 6.2597 - acc: 0.0266\n",
      "Epoch 18/1000\n",
      "978/978 [==============================] - 0s 388us/step - loss: 6.2517 - acc: 0.0266 0s - loss: 6.2945 - acc: 0.02\n",
      "Epoch 19/1000\n",
      "978/978 [==============================] - 0s 326us/step - loss: 6.2427 - acc: 0.0245\n",
      "Epoch 20/1000\n",
      "978/978 [==============================] - 0s 269us/step - loss: 6.2321 - acc: 0.0235\n",
      "Epoch 21/1000\n",
      "978/978 [==============================] - 0s 284us/step - loss: 6.2203 - acc: 0.0225\n",
      "Epoch 22/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 6.2064 - acc: 0.0215\n",
      "Epoch 23/1000\n",
      "978/978 [==============================] - 0s 353us/step - loss: 6.1908 - acc: 0.0266\n",
      "Epoch 24/1000\n",
      "978/978 [==============================] - 0s 295us/step - loss: 6.1736 - acc: 0.0235\n",
      "Epoch 25/1000\n",
      "978/978 [==============================] - 0s 298us/step - loss: 6.1550 - acc: 0.0266\n",
      "Epoch 26/1000\n",
      "978/978 [==============================] - 0s 283us/step - loss: 6.1362 - acc: 0.0245\n",
      "Epoch 27/1000\n",
      "978/978 [==============================] - 0s 300us/step - loss: 6.1186 - acc: 0.0225\n",
      "Epoch 28/1000\n",
      "978/978 [==============================] - 0s 263us/step - loss: 6.1026 - acc: 0.0194\n",
      "Epoch 29/1000\n",
      "978/978 [==============================] - 0s 283us/step - loss: 6.0891 - acc: 0.0225\n",
      "Epoch 30/1000\n",
      "978/978 [==============================] - 0s 336us/step - loss: 6.0775 - acc: 0.0235\n",
      "Epoch 31/1000\n",
      "978/978 [==============================] - 0s 342us/step - loss: 6.0682 - acc: 0.0215\n",
      "Epoch 32/1000\n",
      "978/978 [==============================] - 0s 301us/step - loss: 6.0598 - acc: 0.0204\n",
      "Epoch 33/1000\n",
      "978/978 [==============================] - 0s 309us/step - loss: 6.0526 - acc: 0.0256\n",
      "Epoch 34/1000\n",
      "978/978 [==============================] - 0s 274us/step - loss: 6.0460 - acc: 0.0194\n",
      "Epoch 35/1000\n",
      "978/978 [==============================] - 0s 361us/step - loss: 6.0400 - acc: 0.0245\n",
      "Epoch 36/1000\n",
      "978/978 [==============================] - 0s 306us/step - loss: 6.0347 - acc: 0.0174\n",
      "Epoch 37/1000\n",
      "978/978 [==============================] - 0s 295us/step - loss: 6.0298 - acc: 0.0245\n",
      "Epoch 38/1000\n",
      "978/978 [==============================] - 0s 359us/step - loss: 6.0261 - acc: 0.0215\n",
      "Epoch 39/1000\n",
      "978/978 [==============================] - 0s 352us/step - loss: 6.0220 - acc: 0.0225\n",
      "Epoch 40/1000\n",
      "978/978 [==============================] - 0s 299us/step - loss: 6.0188 - acc: 0.0235\n",
      "Epoch 41/1000\n",
      "978/978 [==============================] - 0s 292us/step - loss: 6.0158 - acc: 0.0184\n",
      "Epoch 42/1000\n",
      "978/978 [==============================] - 0s 287us/step - loss: 6.0129 - acc: 0.0256 0s - loss: 6.0036 - acc: 0.02\n",
      "Epoch 43/1000\n",
      "978/978 [==============================] - 0s 297us/step - loss: 6.0104 - acc: 0.0245\n",
      "Epoch 44/1000\n",
      "978/978 [==============================] - 0s 338us/step - loss: 6.0081 - acc: 0.0215\n",
      "Epoch 45/1000\n",
      "978/978 [==============================] - 0s 317us/step - loss: 6.0064 - acc: 0.0245\n",
      "Epoch 46/1000\n",
      "978/978 [==============================] - 0s 313us/step - loss: 6.0045 - acc: 0.0215\n",
      "Epoch 47/1000\n",
      "978/978 [==============================] - 0s 307us/step - loss: 6.0028 - acc: 0.0256\n",
      "Epoch 48/1000\n",
      "978/978 [==============================] - 0s 321us/step - loss: 6.0014 - acc: 0.0225\n",
      "Epoch 49/1000\n",
      "978/978 [==============================] - 0s 255us/step - loss: 6.0002 - acc: 0.0174\n",
      "Epoch 50/1000\n",
      "978/978 [==============================] - 0s 260us/step - loss: 5.9986 - acc: 0.0215\n",
      "Epoch 51/1000\n",
      "978/978 [==============================] - 0s 268us/step - loss: 5.9973 - acc: 0.0174\n",
      "Epoch 52/1000\n",
      "978/978 [==============================] - 0s 339us/step - loss: 5.9965 - acc: 0.0194\n",
      "Epoch 53/1000\n",
      "978/978 [==============================] - 0s 314us/step - loss: 5.9948 - acc: 0.0256\n",
      "Epoch 54/1000\n",
      "978/978 [==============================] - 0s 262us/step - loss: 5.9938 - acc: 0.0225\n",
      "Epoch 55/1000\n",
      "978/978 [==============================] - 0s 260us/step - loss: 5.9930 - acc: 0.0245\n",
      "Epoch 56/1000\n",
      "978/978 [==============================] - 0s 274us/step - loss: 5.9920 - acc: 0.0235\n",
      "Epoch 57/1000\n",
      "978/978 [==============================] - 0s 254us/step - loss: 5.9915 - acc: 0.0256\n",
      "Epoch 58/1000\n",
      "978/978 [==============================] - 0s 263us/step - loss: 5.9905 - acc: 0.0225\n",
      "Epoch 59/1000\n",
      "978/978 [==============================] - 0s 290us/step - loss: 5.9901 - acc: 0.0235\n",
      "Epoch 60/1000\n",
      "978/978 [==============================] - 0s 310us/step - loss: 5.9891 - acc: 0.0204\n",
      "Epoch 61/1000\n",
      "978/978 [==============================] - 0s 300us/step - loss: 5.9886 - acc: 0.0225\n",
      "Epoch 62/1000\n",
      "978/978 [==============================] - 0s 293us/step - loss: 5.9881 - acc: 0.0225\n",
      "Epoch 63/1000\n",
      "978/978 [==============================] - 0s 286us/step - loss: 5.9871 - acc: 0.0245\n",
      "Epoch 64/1000\n",
      "978/978 [==============================] - 0s 282us/step - loss: 5.9868 - acc: 0.0245\n",
      "Epoch 65/1000\n",
      "978/978 [==============================] - 0s 275us/step - loss: 5.9862 - acc: 0.0204\n",
      "Epoch 66/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 5.9854 - acc: 0.0245\n",
      "Epoch 67/1000\n",
      "978/978 [==============================] - 0s 296us/step - loss: 5.9853 - acc: 0.0194\n",
      "Epoch 68/1000\n",
      "978/978 [==============================] - 0s 279us/step - loss: 5.9853 - acc: 0.0184\n",
      "Epoch 69/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 5.9841 - acc: 0.0245\n",
      "Epoch 70/1000\n",
      "978/978 [==============================] - 0s 280us/step - loss: 5.9840 - acc: 0.0245\n",
      "Epoch 71/1000\n",
      "978/978 [==============================] - 0s 293us/step - loss: 5.9836 - acc: 0.0245\n",
      "Epoch 72/1000\n",
      "978/978 [==============================] - 0s 293us/step - loss: 5.9831 - acc: 0.0235\n",
      "Epoch 73/1000\n",
      "978/978 [==============================] - 0s 280us/step - loss: 5.9827 - acc: 0.0245\n",
      "Epoch 74/1000\n",
      "978/978 [==============================] - 0s 294us/step - loss: 5.9823 - acc: 0.0204\n",
      "Epoch 75/1000\n",
      "978/978 [==============================] - 0s 290us/step - loss: 5.9823 - acc: 0.0215\n",
      "Epoch 76/1000\n",
      "978/978 [==============================] - 0s 291us/step - loss: 5.9824 - acc: 0.0194\n",
      "Epoch 77/1000\n",
      "978/978 [==============================] - 0s 302us/step - loss: 5.9817 - acc: 0.0245\n",
      "Epoch 78/1000\n",
      "978/978 [==============================] - 0s 301us/step - loss: 5.9812 - acc: 0.0225\n",
      "Epoch 79/1000\n",
      "978/978 [==============================] - ETA: 0s - loss: 5.9844 - acc: 0.020 - 0s 287us/step - loss: 5.9810 - acc: 0.0225\n",
      "Epoch 80/1000\n",
      "978/978 [==============================] - 0s 264us/step - loss: 5.9808 - acc: 0.0245\n",
      "Epoch 81/1000\n",
      "978/978 [==============================] - 0s 261us/step - loss: 5.9799 - acc: 0.0256\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978/978 [==============================] - 0s 277us/step - loss: 1.3901 - acc: 0.8231\n",
      "Epoch 965/1000\n",
      "978/978 [==============================] - 0s 361us/step - loss: 1.3788 - acc: 0.8272\n",
      "Epoch 966/1000\n",
      "978/978 [==============================] - 0s 312us/step - loss: 1.3751 - acc: 0.8333\n",
      "Epoch 967/1000\n",
      "978/978 [==============================] - 0s 363us/step - loss: 1.3718 - acc: 0.8323\n",
      "Epoch 968/1000\n",
      "978/978 [==============================] - 0s 368us/step - loss: 1.3724 - acc: 0.8344\n",
      "Epoch 969/1000\n",
      "978/978 [==============================] - 0s 338us/step - loss: 1.3702 - acc: 0.8292\n",
      "Epoch 970/1000\n",
      "978/978 [==============================] - 0s 351us/step - loss: 1.3638 - acc: 0.8323\n",
      "Epoch 971/1000\n",
      "978/978 [==============================] - 0s 359us/step - loss: 1.3569 - acc: 0.8354\n",
      "Epoch 972/1000\n",
      "978/978 [==============================] - 0s 341us/step - loss: 1.3578 - acc: 0.8323\n",
      "Epoch 973/1000\n",
      "978/978 [==============================] - 0s 341us/step - loss: 1.3551 - acc: 0.8313\n",
      "Epoch 974/1000\n",
      "978/978 [==============================] - 0s 338us/step - loss: 1.3494 - acc: 0.8354\n",
      "Epoch 975/1000\n",
      "978/978 [==============================] - 0s 327us/step - loss: 1.3419 - acc: 0.8374\n",
      "Epoch 976/1000\n",
      "978/978 [==============================] - 0s 348us/step - loss: 1.3387 - acc: 0.8456\n",
      "Epoch 977/1000\n",
      "978/978 [==============================] - 0s 339us/step - loss: 1.3362 - acc: 0.8436\n",
      "Epoch 978/1000\n",
      "978/978 [==============================] - 0s 342us/step - loss: 1.3345 - acc: 0.8395\n",
      "Epoch 979/1000\n",
      "978/978 [==============================] - 0s 323us/step - loss: 1.3296 - acc: 0.8476\n",
      "Epoch 980/1000\n",
      "978/978 [==============================] - 0s 345us/step - loss: 1.3270 - acc: 0.8415\n",
      "Epoch 981/1000\n",
      "978/978 [==============================] - 0s 310us/step - loss: 1.3226 - acc: 0.8466\n",
      "Epoch 982/1000\n",
      "978/978 [==============================] - 0s 341us/step - loss: 1.3174 - acc: 0.8476\n",
      "Epoch 983/1000\n",
      "978/978 [==============================] - 0s 285us/step - loss: 1.3153 - acc: 0.8436\n",
      "Epoch 984/1000\n",
      "978/978 [==============================] - 0s 336us/step - loss: 1.3116 - acc: 0.8436\n",
      "Epoch 985/1000\n",
      "978/978 [==============================] - 0s 345us/step - loss: 1.3139 - acc: 0.8538\n",
      "Epoch 986/1000\n",
      "978/978 [==============================] - 0s 328us/step - loss: 1.3051 - acc: 0.8507\n",
      "Epoch 987/1000\n",
      "978/978 [==============================] - 0s 301us/step - loss: 1.3023 - acc: 0.8466\n",
      "Epoch 988/1000\n",
      "978/978 [==============================] - 0s 277us/step - loss: 1.2974 - acc: 0.8497\n",
      "Epoch 989/1000\n",
      "978/978 [==============================] - 0s 283us/step - loss: 1.2942 - acc: 0.8528\n",
      "Epoch 990/1000\n",
      "978/978 [==============================] - 0s 310us/step - loss: 1.2902 - acc: 0.8558\n",
      "Epoch 991/1000\n",
      "978/978 [==============================] - 0s 307us/step - loss: 1.2907 - acc: 0.8569\n",
      "Epoch 992/1000\n",
      "978/978 [==============================] - 0s 347us/step - loss: 1.2862 - acc: 0.8487\n",
      "Epoch 993/1000\n",
      "978/978 [==============================] - 0s 351us/step - loss: 1.2796 - acc: 0.8589\n",
      "Epoch 994/1000\n",
      "978/978 [==============================] - 0s 329us/step - loss: 1.2753 - acc: 0.8548\n",
      "Epoch 995/1000\n",
      "978/978 [==============================] - 0s 337us/step - loss: 1.2728 - acc: 0.8589\n",
      "Epoch 996/1000\n",
      "978/978 [==============================] - 0s 315us/step - loss: 1.2732 - acc: 0.8579\n",
      "Epoch 997/1000\n",
      "978/978 [==============================] - 0s 328us/step - loss: 1.2658 - acc: 0.8620\n",
      "Epoch 998/1000\n",
      "978/978 [==============================] - 0s 358us/step - loss: 1.2633 - acc: 0.8558\n",
      "Epoch 999/1000\n",
      "978/978 [==============================] - 0s 333us/step - loss: 1.2589 - acc: 0.8650 0s - loss: 1.2497 - acc: 0.870\n",
      "Epoch 1000/1000\n",
      "978/978 [==============================] - 0s 307us/step - loss: 1.2581 - acc: 0.8569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1636e61f60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=1000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['굳다', 'ㅋ', 'GDNTOPCLASSINTHECLUB']===>277 보다\n",
      "['ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐']===>450 이\n",
      "['GDNTOPCLASSINTHECLUB', '뭐', '이']===>585 평점\n",
      "['뭐', '이', '평점']===>185 들다\n",
      "['이', '평점', '들']===>119 나쁘다\n",
      "['평점', '들', '나쁘다']===>374 않다\n",
      "['들', '나쁘다', '않다']===>1 10\n",
      "['나쁘다', '않다', '10']===>496 점\n",
      "['않다', '10', '점']===>539 짜다\n",
      "['10', '점', '짜다']===>55 개막\n",
      "['점', '짜다', '리']===>165 더\n",
      "['짜다', '리', '더']===>166 더욱\n",
      "['리', '더', '더욱']===>356 아니다\n",
      "['더', '더욱', '아니다']===>528 지루하다\n",
      "['더욱', '아니다', '지루하다']===>374 않다\n",
      "['아니다', '지루하다', '않다']===>375 알\n",
      "['지루하다', '않다', '완전']===>488 재밌다\n",
      "['않다', '완전', '막장']===>473 임\n",
      "['완전', '막장', '임']===>172 돈\n",
      "['막장', '임', '돈']===>474 있다\n",
      "['임', '돈', '주다']===>276 보기\n",
      "['돈', '주다', '보기']===>10 3\n",
      "['주다', '보기', '3']===>20 d\n",
      "['보기', '3', 'D']===>220 만\n",
      "['3', 'D', '만']===>296 빠지다\n",
      "['D', '만', '아니다']===>273 별\n",
      "['만', '아니다', '별']===>152 다섯\n",
      "['아니다', '별', '다섯']===>53 개\n",
      "['별', '다섯', '개']===>515 주다\n",
      "['다섯', '개', '주다']===>426 왜\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(tokenized_words[i: i+3], end='===>')\n",
    "    print(pre[i].argmax(), end=' ')\n",
    "    print(vect.get_feature_names()[pre[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
